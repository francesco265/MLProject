{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Useful libraries"
      ],
      "metadata": {
        "id": "7tL1BQLTtDzf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lU_v7C3cJS0"
      },
      "outputs": [],
      "source": [
        "! pip install dgl -f https://data.dgl.ai/wheels/cu118/repo.html\n",
        "! pip install dglgo -f https://data.dgl.ai/wheels-test/repo.htm\n",
        "import dgl\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import sklearn.metrics as m\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "c9LYmOgXtIpD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ag1QDtdkbuk7"
      },
      "outputs": [],
      "source": [
        "class GNCN(nn.Module):\n",
        "  def __init__(self, in_features, out_features, s=1.8, activation=lambda x: x, dropout=0):\n",
        "    super().__init__()\n",
        "    self.drop = nn.Dropout(dropout)\n",
        "    self.weights = nn.Linear(in_features, out_features, bias=False)\n",
        "    self.act = activation\n",
        "    self.s = s\n",
        "  def reset_parameters(self):\n",
        "    self.weights.reset_parameters()\n",
        "  def forward(self, A, X):\n",
        "    X = self.drop(X)\n",
        "    X = F.normalize(self.weights(X))\n",
        "    return self.act(self.s * A.mm(X))\n",
        "\n",
        "class InputEncoder(nn.Module):\n",
        "  def __init__(self, feat_dim, emb_dim, pe_enc=False, device='cpu'):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(feat_dim, emb_dim)\n",
        "    self.pe_enc = pe_enc\n",
        "    self.device = device\n",
        "    self.init = True if self.pe_enc else False\n",
        "  def reset_parameters(self):\n",
        "    self.init = True if self.pe_enc else False\n",
        "    for layer in self.children():\n",
        "      if hasattr(layer, 'reset_parameters'):\n",
        "        layer.reset_parameters()\n",
        "  def forward(self, A, X):\n",
        "    if self.init:\n",
        "      print(\"Computing laplacian positional embeddings...\")\n",
        "      self.pe = laplacian_pe(A, self.linear.out_features, self.device)\n",
        "      self.init = False\n",
        "    if self.pe_enc:\n",
        "      return self.linear(X) + self.pe\n",
        "    else:\n",
        "      return self.linear(X)\n",
        "\n",
        "class GTAE(nn.Module):\n",
        "  def __init__(self, feat_dim, emb_dim, latent_size, n_heads, n_layers=2, pe_enc=False, device='cpu'):\n",
        "    super().__init__()\n",
        "    self.input = InputEncoder(feat_dim, emb_dim, pe_enc, device)\n",
        "    t_enc = nn.TransformerEncoderLayer(emb_dim, n_heads, emb_dim, activation=F.gelu, dropout=0.1)\n",
        "    self.t_enc_stack = nn.TransformerEncoder(t_enc, n_layers)\n",
        "    self.linear = nn.Linear(emb_dim, latent_size)\n",
        "  def reset_parameters(self):\n",
        "    for layer in self.children():\n",
        "      if hasattr(layer, 'reset_parameters'):\n",
        "        layer.reset_parameters()\n",
        "  def forward(self, A, X):\n",
        "    self.enc = self.linear(self.t_enc_stack(self.input(A, X), A))\n",
        "    return F.sigmoid(self.enc.mm(self.enc.T))\n",
        "\n",
        "class GCAE(nn.Module):\n",
        "  def __init__(self, in_feat, hid_size, latent_size):\n",
        "    super().__init__()\n",
        "    self.conv1 = dgl.nn.GraphConv(in_feat, hid_size, activation=F.relu)\n",
        "    self.conv2 = dgl.nn.GraphConv(hid_size, latent_size)\n",
        "    self.proj = nn.Sequential(\n",
        "        nn.Linear(latent_size, latent_size * 2, bias=False),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(latent_size * 2, latent_size, bias=False)\n",
        "    )\n",
        "  def reset_parameters(self):\n",
        "    for layer in self.children():\n",
        "      if hasattr(layer, 'reset_parameters'):\n",
        "        layer.reset_parameters()\n",
        "  def forward(self, A, X, need_proj=False):\n",
        "    self.enc = self.conv2(A, self.conv1(A, X))\n",
        "    if need_proj:\n",
        "      return F.sigmoid(self.enc.mm(self.enc.T)), self.proj(self.enc)\n",
        "    else:\n",
        "      return F.sigmoid(self.enc.mm(self.enc.T))\n",
        "\n",
        "class GATAE(nn.Module):\n",
        "  def __init__(self, in_feat, hid_size, latent_size, n_heads1, n_heads2):\n",
        "    super().__init__()\n",
        "    if hid_size % n_heads1 != 0:\n",
        "      return Exception(f\"hid_size ({hid_size}) must be divisible by n_heads1 ({n_heads1})\")\n",
        "    self.conv1 = dgl.nn.GATConv(in_feat, int(hid_size / n_heads1), n_heads1, activation=F.elu)\n",
        "    self.conv2 = dgl.nn.GATConv(hid_size, latent_size, n_heads2)\n",
        "    self.proj = nn.Sequential(\n",
        "        nn.Linear(latent_size, latent_size * 2, bias=False),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(latent_size * 2, latent_size, bias=False)\n",
        "    )\n",
        "  def reset_parameters(self):\n",
        "    for layer in self.children():\n",
        "      if hasattr(layer, 'reset_parameters'):\n",
        "        layer.reset_parameters()\n",
        "  def forward(self, A, X, need_proj=False):\n",
        "    # In the first attention layer head's output are concatenated, in the second\n",
        "    # they're averaged\n",
        "    out, att = self.conv1(A, X, get_attention=True)\n",
        "    self.enc = self.conv2(A, out.flatten(1)).mean(1)\n",
        "    if need_proj:\n",
        "      return F.sigmoid(self.enc.mm(self.enc.T)), self.proj(self.enc)\n",
        "    else:\n",
        "      return F.sigmoid(self.enc.mm(self.enc.T)), att\n",
        "\n",
        "class GNCAE(nn.Module):\n",
        "  def __init__(self, in_feat, hid_size, latent_size, s=1.8):\n",
        "    super().__init__()\n",
        "    self.conv1 = GNCN(in_feat, hid_size, s=s, activation=F.relu)\n",
        "    self.conv2 = GNCN(hid_size, latent_size, s=s)\n",
        "    self.proj = nn.Sequential(\n",
        "        nn.Linear(latent_size, latent_size * 2, bias=False),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(latent_size * 2, latent_size, bias=False)\n",
        "    )\n",
        "  def reset_parameters(self):\n",
        "    for layer in self.children():\n",
        "      if hasattr(layer, 'reset_parameters'):\n",
        "        layer.reset_parameters()\n",
        "  def normalize(self, A):\n",
        "    # Add self loops\n",
        "    A += torch.eye(A.shape[0], out=torch.empty_like(A))\n",
        "    # Compute degree matrix\n",
        "    D = A.sum(axis=1).pow(-0.5)\n",
        "    return (D[None,:].T * A * D).to_sparse()\n",
        "  def forward(self, A, X, need_proj=False):\n",
        "    A = self.normalize(A)\n",
        "    self.enc = self.conv2(A, self.conv1(A, X))\n",
        "    if need_proj:\n",
        "      return F.sigmoid(self.enc.mm(self.enc.T)), self.proj(self.enc)\n",
        "    else:\n",
        "      return F.sigmoid(self.enc.mm(self.enc.T))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing"
      ],
      "metadata": {
        "id": "Jy6JfR6itkJj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-I203ybFngQ"
      },
      "outputs": [],
      "source": [
        "def sparse_to_tuple(sparse_mx):\n",
        "    if not sp.isspmatrix_coo(sparse_mx):\n",
        "        sparse_mx = sparse_mx.tocoo()\n",
        "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
        "    return coords\n",
        "\n",
        "def ismember(a, b):\n",
        "    return np.any(np.all((a - b) == 0, axis=-1))\n",
        "\n",
        "# Splits data in train and test, returns an iterator to the k folds\n",
        "def kfold(adj, k=10, valid=0.05):\n",
        "    all_edges = sparse_to_tuple(adj)\n",
        "    edges = sparse_to_tuple(sp.triu(adj))\n",
        "    n_edges = edges.shape[0]\n",
        "    valid_size = int(n_edges * valid)\n",
        "\n",
        "    edges_idx = np.arange(n_edges)\n",
        "    np.random.shuffle(edges_idx)\n",
        "    k_train_folds = np.array_split(edges_idx, k)\n",
        "\n",
        "    # build negative samples, one for each fold (used only for testing)\n",
        "    neg_edges = np.empty((0,2), dtype=int)\n",
        "    while len(neg_edges) < n_edges + valid_size * k:\n",
        "      i = np.random.randint(0, adj.shape[0])\n",
        "      j = np.random.randint(0, adj.shape[0])\n",
        "      if ismember([i, j], all_edges):\n",
        "          continue\n",
        "      if ismember([i, j], neg_edges) or ismember([j, i], neg_edges):\n",
        "          continue\n",
        "      neg_edges = np.vstack((neg_edges, [i, j]))\n",
        "    test_neg_edges = np.array_split(neg_edges[:n_edges], k)\n",
        "    val_neg_edges = np.array_split(neg_edges[n_edges:], k)\n",
        "\n",
        "    # positive samples for train, test and validation\n",
        "    for i, test_edges_idx in enumerate(k_train_folds):\n",
        "      train_edges_idx = np.hstack(np.delete(k_train_folds, i, axis=0))\n",
        "      np.random.shuffle(train_edges_idx)\n",
        "\n",
        "      test_edges = edges[test_edges_idx]\n",
        "      val_edges = edges[train_edges_idx[:valid_size]]\n",
        "      train_edges = edges[train_edges_idx[valid_size:]]\n",
        "\n",
        "      # build train adjacency matric\n",
        "      adj_train = sp.csr_matrix((np.ones(len(train_edges)), (train_edges[:,0], train_edges[:,1])), adj.shape, dtype=adj.dtype)\n",
        "      adj_train = adj_train + adj_train.T\n",
        "      yield adj_train, train_edges, test_edges, test_neg_edges[i], val_edges, val_neg_edges[i]\n",
        "\n",
        "# Split data in train, test, validation\n",
        "def split_data(adj, test=0.1, valid=0.05):\n",
        "    all_edges = sparse_to_tuple(adj)\n",
        "    edges = sparse_to_tuple(sp.triu(adj))\n",
        "    # 10% test and 5% validation\n",
        "    n_test = int(edges.shape[0] * test)\n",
        "    n_val = int(edges.shape[0] * valid)\n",
        "\n",
        "    edges_idx = np.arange(edges.shape[0])\n",
        "    np.random.shuffle(edges_idx)\n",
        "    val_edges = edges[edges_idx[:n_val]]\n",
        "    test_edges = edges[edges_idx[n_val:(n_val + n_test)]]\n",
        "    train_edges = edges[edges_idx[(n_val + n_test):]]\n",
        "\n",
        "    edges_false = np.empty((0,2), dtype=int)\n",
        "    while len(edges_false) < len(test_edges) + len(val_edges):\n",
        "        i = np.random.randint(0, adj.shape[0])\n",
        "        j = np.random.randint(0, adj.shape[0])\n",
        "        if ismember([i, j], all_edges):\n",
        "            continue\n",
        "        if ismember([i, j], edges_false) or ismember([j, i], edges_false):\n",
        "            continue\n",
        "        edges_false = np.vstack((edges_false, [i, j]))\n",
        "    test_edges_false, val_edges_false = edges_false[:len(test_edges)], edges_false[len(test_edges):]\n",
        "\n",
        "    adj_train = sp.csr_matrix((np.ones(len(train_edges)), (train_edges[:,0], train_edges[:,1])), adj.shape, dtype=adj.dtype)\n",
        "    adj_train = adj_train + adj_train.T\n",
        "\n",
        "    return adj_train, train_edges, test_edges, test_edges_false, val_edges, val_edges_false"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reconstruction based learning\n",
        "Here is implemented:\n",
        "- Metrics calculation (AUC, AP, MRR, MR and Hits)\n",
        "- k-fold training"
      ],
      "metadata": {
        "id": "aMQCbCzDt5OC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ww58JICN7Ls3"
      },
      "outputs": [],
      "source": [
        "def compute_ranking_metrics(true, pred, edges, neg_sampling=100):\n",
        "  n = true.shape[0]\n",
        "  # add self loops\n",
        "  true = true + torch.eye(n)\n",
        "\n",
        "  hits1 = 0\n",
        "  hits3 = 0\n",
        "  hits10 = 0\n",
        "  mr = 0\n",
        "  mrr = 0\n",
        "  for pos in edges:\n",
        "    scores = [pred[tuple(pos)]]\n",
        "    i = np.random.choice([0,1])\n",
        "    for j in np.random.choice(np.arange(n)[true[i] == 0], neg_sampling, False):\n",
        "      neg = pos.copy()\n",
        "      neg[(i-1)%2] = j\n",
        "      scores.append(pred[tuple(neg)])\n",
        "    rank = float(np.argsort(scores)[::-1].argmin() + 1)\n",
        "    hits10 += 1 if rank <= 10 else 0\n",
        "    hits3 += 1 if rank <= 3 else 0\n",
        "    hits1 += 1 if rank == 1 else 0\n",
        "    mr += rank\n",
        "    mrr += 1 / rank\n",
        "\n",
        "  metrics = {'mrr': mrr / len(edges),\n",
        "             'mr': mr / len(edges),\n",
        "             'hits1': hits1 / len(edges),\n",
        "             'hits3': hits3 / len(edges),\n",
        "             'hits10': hits10 / len(edges)}\n",
        "  return metrics\n",
        "\n",
        "def compute_metrics(pos_true, neg_true, preds):\n",
        "  n = pos_true.shape[0]\n",
        "  pos_pred = []\n",
        "  neg_pred = []\n",
        "  for i in range(n):\n",
        "    pos_pred.append(preds[tuple(pos_true[i])])\n",
        "    neg_pred.append(preds[tuple(neg_true[i])])\n",
        "  p = np.hstack((np.asarray(pos_pred), np.asarray(neg_pred)))\n",
        "  t = np.hstack((np.ones(n), np.zeros(n)))\n",
        "\n",
        "  metrics = {'loss': float(m.log_loss(t, p)),\n",
        "             'ap': float(m.average_precision_score(t, p)),\n",
        "             'auc': float(m.roc_auc_score(t, p))}\n",
        "  return metrics\n",
        "\n",
        "def train_kfold(\n",
        "    model,            # Model to train\n",
        "    A,                # Graph adjacency matrix\n",
        "    X,                # Node's features\n",
        "    k=10,             # K fold\n",
        "    lr=1e-2,          # Learning rate\n",
        "    wd=1e-5,          # Weight decay\n",
        "    valid_size=0.05,  # Validation set size\n",
        "    epochs_max=300,   # Max number of epochs\n",
        "    patience_max=30,  # Max number of epochs to wait before early stopping (if no improvements)\n",
        "    dgl_model=True,   # Must be True if the model is implemented via DGL library, False otherwise\n",
        "    device='cpu'):\n",
        "  \"\"\"\n",
        "  Train a specific model using k-fold cross validation, returns a list of metrics\n",
        "  computed on each fold\n",
        "  \"\"\"\n",
        "  fold = 1\n",
        "  metrics = []\n",
        "  for A_train, train_e, test_e, test_ef, val_e, val_ef in kfold(A, k, valid_size):\n",
        "    # reset weights\n",
        "    model.reset_parameters()\n",
        "    if dgl_model:\n",
        "      A_train_dgl = dgl.from_scipy(A_train)\n",
        "      A_train_dgl = A_train_dgl.add_self_loop()\n",
        "\n",
        "    # scipy sparse matrix -> torch dense tensor\n",
        "    n = A_train.sum()\n",
        "    A_train = torch.from_numpy(A_train.todense()).to(device)\n",
        "    pos_w = (A_train.shape[0] * A_train.shape[0] - n) / n\n",
        "    norm = A_train.shape[0] * A_train.shape[0] / ((A_train.shape[0] * A_train.shape[0] - n) * 2)\n",
        "    A_train_w = A_train.clone().flatten()\n",
        "    A_train_w[A_train_w == 1] = pos_w\n",
        "    A_train_w[A_train_w == 0] = 1\n",
        "\n",
        "    model.train()\n",
        "    losses = {'train_loss': [], 'valid_loss': []}\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    loss = nn.BCELoss(weight=A_train_w)\n",
        "    patience_act = 0\n",
        "    min_loss = np.inf\n",
        "    for epoch in range(epochs_max):\n",
        "      if patience_act >= patience_max:\n",
        "        print(f'[fold {fold}] max patience reached after {patience_max} epochs, training stopped')\n",
        "        break\n",
        "      model.zero_grad()\n",
        "      out = model(A_train_dgl if dgl_model else A_train, X)\n",
        "      l = norm * loss(out.flatten(), A_train.flatten())\n",
        "      l.backward()\n",
        "      opt.step()\n",
        "      val_metrics = compute_metrics(val_e, val_ef, out.detach().cpu())\n",
        "      if val_metrics['loss'] < min_loss:\n",
        "        min_loss = val_metrics['loss']\n",
        "        model_state = model.state_dict()\n",
        "        patience_act = 0\n",
        "      elif epoch > 50:\n",
        "        patience_act += 1\n",
        "      if epoch % 10 == 0:\n",
        "        losses['train_loss'].append(np.float16(l.item()))\n",
        "        losses['valid_loss'].append(np.float16(val_metrics['loss']))\n",
        "      print(f'[fold {fold}] epoch {epoch}: train loss = {l:.4f}, valid loss = {val_metrics[\"loss\"]:.4f}, valid AUC = {val_metrics[\"auc\"]:.4f}')\n",
        "\n",
        "    # Load best model based on validation set metrics\n",
        "    model.eval()\n",
        "    model.load_state_dict(model_state)\n",
        "    with torch.no_grad():\n",
        "      out = model(A_train_dgl if dgl_model else A_train, X).cpu()\n",
        "      test_metrics = compute_metrics(test_e, test_ef, out)\n",
        "      test_ranking = compute_ranking_metrics(torch.Tensor(A.todense()), out, test_e)\n",
        "    print(f'[fold {fold}] test metrics: {test_metrics}')\n",
        "    print(f'[fold {fold}] test mrr: {test_ranking}')\n",
        "    test_metrics.update(test_ranking)\n",
        "    test_metrics.update(losses)\n",
        "    metrics.append(test_metrics)\n",
        "    fold += 1\n",
        "  return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAVv9gvdZy2K"
      },
      "source": [
        "## Run experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDVEjKCOZpi0"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import json\n",
        "dev = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "DATASET = 'facebook'\n",
        "if DATASET == 'facebook':\n",
        "  with open('facebook.json') as f:\n",
        "    data = json.load(f)\n",
        "  X = torch.Tensor(data['feat'])\n",
        "  edges = np.asarray(data['edges'])\n",
        "  A = sp.csr_matrix((np.ones(len(edges)), (edges[:,0], edges[:,1])), (X.shape[0], X.shape[0]), dtype=np.float32)\n",
        "else:\n",
        "  if DATASET == 'cora':\n",
        "    g = dgl.data.CoraGraphDataset()[0]\n",
        "  elif DATASET == 'citeseer':\n",
        "    g = dgl.data.CiteseerGraphDataset()[0]\n",
        "  elif DATASET == 'pubmed':\n",
        "    g = dgl.data.PubmedGraphDataset()[0]\n",
        "  # adjacency matrix\n",
        "  A = g.adj_external(scipy_fmt='csr').astype(np.float32)\n",
        "  # feature vectors\n",
        "  X = g.ndata['feat'].to(dev)\n",
        "\n",
        "m1 = GCAE(X.shape[1], 128, 64)\n",
        "m2 = GNCAE(X.shape[1], 128, 64, s=1.8)\n",
        "m3 = GATAE(X.shape[1], 128, 64, 4, 1)\n",
        "m4 = GTAE(X.shape[1], 64, 32, 4, n_layers=1, pe_enc=True, device=dev)\n",
        "for model in [m1]:\n",
        "  model = model.to(dev)\n",
        "  metrics = train_kfold(model, A, X, epochs_max=500, dgl_model=True, dev=dev)\n",
        "  with open(f'{DATASET}_{model._get_name()}_metrics.pkl', 'wb') as f:\n",
        "    pickle.dump(metrics, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJheRCERot1l"
      },
      "source": [
        "# Reconstruction + contrastive learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqjTdqsxotj8"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "from math import comb\n",
        "\n",
        "def contrastive_loss(x_base, x_aug, temp=1.0, device='cpu'):\n",
        "  proj_size = x_base.shape[0]\n",
        "  mask = (torch.diag(torch.full((proj_size,), -np.inf), proj_size) + torch.diag(torch.full((proj_size,), -np.inf), -proj_size)).to(device)\n",
        "  boh1 = torch.vstack((x_aug, x_base))\n",
        "  boh2 = torch.vstack((x_base, x_aug))\n",
        "  sim = F.normalize(boh1).mm(F.normalize(boh2).T)\n",
        "  sim /= temp\n",
        "  return -F.log_softmax(sim + mask, dim=1).diag().mean()\n",
        "\n",
        "def train_contrastive_kfold(\n",
        "    model,                  # Model to train\n",
        "    A,                      # Graph adjacency matrix\n",
        "    X,                      # Node's features\n",
        "    part_type='clust_conj', # Type of partitioning to use\n",
        "    n_part=2,               # Number of partitions\n",
        "    contrastive_w=0.5,      # Weight coefficient given to the contrastive loss\n",
        "    contrastive_t=0.5,      # Temperature used in contrastive loss softmax\n",
        "    proj_size=64,           # Dimension of the node's projections, used only if part_type == 'clust_dis'\n",
        "    k=10,                   # K fold\n",
        "    lr=1e-2,                # Learning rate\n",
        "    wd=1e-5,                # Weight decay\n",
        "    valid_size=0.05,        # Validation set size\n",
        "    epochs_max=300,         # Max number of epochs\n",
        "    patience_max=30,        # Max number of epochs to wait before early stopping (if no improvements are made)\n",
        "    dgl_model=True,         # Must be True if the model is implemented via DGL library, False otherwise\n",
        "    device='cpu'):\n",
        "  if part_type not in ['res', 'clust_dis', 'clust_conj']:\n",
        "    return Exception('invalid partitioning type')\n",
        "\n",
        "  fold = 1\n",
        "  metrics = []\n",
        "  for A_train, train_e, test_e, test_ef, val_e, val_ef in kfold(A, k, valid_size):\n",
        "    # reset weights\n",
        "    model.reset_parameters()\n",
        "\n",
        "    # static partitioning\n",
        "    if part_type == 'clust_dis':\n",
        "      subA, subA_nodes = partition(A_train, n_part)\n",
        "    elif part_type == 'clust_conj':\n",
        "      subA, subA_nodes = partition(A_train, n_part, remove_edges=False)\n",
        "    else:\n",
        "      subA = random_edge_splitting(A_train, n_part)\n",
        "\n",
        "    subA_dgl = []\n",
        "    if dgl_model:\n",
        "      for i in subA:\n",
        "        tmp = dgl.from_scipy(i).to(device)\n",
        "        tmp = tmp.add_self_loop()\n",
        "        subA_dgl.append(tmp)\n",
        "\n",
        "    norm = []\n",
        "    loss = []\n",
        "    for i in range(len(subA)):\n",
        "      subA[i] = torch.from_numpy(subA[i].todense()).to(device)\n",
        "      n = subA[i].sum()\n",
        "      pos_w = (subA[i].shape[0] * subA[i].shape[0] - n) / n\n",
        "      norm.append(subA[i].shape[0] * subA[i].shape[0] / ((subA[i].shape[0] * subA[i].shape[0] - n) * 2))\n",
        "      tmp = subA[i].clone().flatten()\n",
        "      tmp[tmp == 1] = pos_w\n",
        "      tmp[tmp == 0] = 1\n",
        "      loss.append(nn.BCELoss(weight=tmp))\n",
        "\n",
        "    model.train()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    patience_act = 0\n",
        "    min_loss = np.inf\n",
        "    contrastive_active = False\n",
        "    for epoch in range(epochs_max):\n",
        "      if patience_act >= patience_max:\n",
        "        print(f'[fold {fold}] max patience reached after {patience_max} epochs, training stopped')\n",
        "        break\n",
        "      if epoch > 50:\n",
        "        contrastive_active = True\n",
        "      model.zero_grad()\n",
        "      embeddings = []\n",
        "      l = 0\n",
        "      if part_type == 'clust_dis':\n",
        "        proj_base = torch.empty((A.shape[0], proj_size))\n",
        "        proj_aug = torch.empty((A.shape[0], proj_size))\n",
        "      else:\n",
        "        projs = []\n",
        "\n",
        "      for i in range(len(subA)):\n",
        "        out, proj = model(subA_dgl[i] if dgl_model else subA[i], X, need_proj=True)\n",
        "        embeddings.append(model.enc)\n",
        "        l += norm[i] * loss[i](out.flatten(), subA[i].flatten())\n",
        "\n",
        "        if part_type == 'clust_dis':\n",
        "          subA_not = np.hstack(np.delete(subA_nodes, i, axis=0))\n",
        "          ## Augmented projections\n",
        "          proj_aug[subA_nodes[i],:] = proj[subA_nodes[i],:]\n",
        "          ## Base projections\n",
        "          proj_base[subA_not,:] = proj[subA_not[i],:]\n",
        "        else:\n",
        "          projs.append(proj)\n",
        "      l /= n_part\n",
        "\n",
        "      # contrastive loss\n",
        "      if contrastive_active:\n",
        "        if part_type == 'clust_dis':\n",
        "          lc = contrastive_loss(proj_base, proj_aug, contrastive_t, device=device)\n",
        "        else:\n",
        "          lc = 0\n",
        "          for c in combinations(projs, 2):\n",
        "            lc += contrastive_loss(*c, contrastive_t, device=device)\n",
        "          lc /= comb(n_part, 2)\n",
        "        l += contrastive_w * lc\n",
        "\n",
        "      l.backward()\n",
        "      opt.step()\n",
        "\n",
        "      embeddings = torch.stack(embeddings).mean(dim=0)\n",
        "      out = F.sigmoid(embeddings.mm(embeddings.T))\n",
        "      val_metrics = compute_metrics(val_e, val_ef, out.detach().cpu())\n",
        "      if val_metrics['loss'] < min_loss:\n",
        "        min_loss = val_metrics['loss']\n",
        "        model_state = model.state_dict()\n",
        "        patience_act = 0\n",
        "      elif epoch > 50:\n",
        "        patience_act += 1\n",
        "      print(f'[fold {fold}] epoch {epoch}: train loss = {l:.4f}, valid loss = {val_metrics[\"loss\"]:.4f}, valid AUC = {val_metrics[\"auc\"]:.4f}')\n",
        "\n",
        "    # Load best model based on validation set metrics\n",
        "    model.eval()\n",
        "    model.load_state_dict(model_state)\n",
        "    with torch.no_grad():\n",
        "      embeddings = []\n",
        "      for i in range(len(subA)):\n",
        "        out = model(subA_dgl[i] if dgl_model else subA[i], X)\n",
        "        embeddings.append(model.enc)\n",
        "      embeddings = torch.stack(embeddings).mean(dim=0)\n",
        "      out = F.sigmoid(embeddings.mm(embeddings.T))\n",
        "      test_metrics = compute_metrics(test_e, test_ef, out.cpu())\n",
        "      test_ranking = compute_ranking_metrics(torch.Tensor(A.todense()), out.cpu(), test_e)\n",
        "    print(f'[fold {fold}] test metrics: {test_metrics}')\n",
        "    print(f'[fold {fold}] test mrr: {test_ranking}')\n",
        "    test_metrics.update(test_ranking)\n",
        "    metrics.append(test_metrics)\n",
        "    fold += 1\n",
        "  return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96fLE6zRoZNx"
      },
      "source": [
        "### Partitioning\n",
        "- Using Fiedler eigenvector\n",
        "- Random edge splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fkUihfCmksk"
      },
      "outputs": [],
      "source": [
        "def laplacian_eig(adj, norm=False, device='cpu'):\n",
        "  if not torch.is_tensor(adj):\n",
        "    adj = torch.Tensor(adj.todense())\n",
        "  if norm:\n",
        "    D = torch.diag(adj.sum(axis=1).clip(1).pow(-0.5))\n",
        "    L = torch.eye(adj.shape[0]).to(device) - D.mm(adj.mm(D))\n",
        "  else:\n",
        "    D = torch.diag(adj.sum(axis=1))\n",
        "    L = D - adj\n",
        "\n",
        "  L = L.cpu()\n",
        "  eig, eiv = map(lambda x: x.astype(float), np.linalg.eig(L))\n",
        "  eig[eig < 1e-10] = 0\n",
        "  return eig, np.asarray(eiv)\n",
        "\n",
        "def laplacian_pe(adj, k, device='cpu'):\n",
        "  eig, eiv = laplacian_eig(adj, norm=True, device=device)\n",
        "  eiv = eiv[:, eig > 0]\n",
        "  eig = eig[eig > 0]\n",
        "  sort = np.argsort(eig)\n",
        "  sign = 2 * (np.random.rand(k) > 0.5) - 1\n",
        "  return torch.Tensor(sign * np.real(eiv[:, sort[:k]])).to(device)\n",
        "\n",
        "def partition(adj, k, remove_edges=True, device='cpu'):\n",
        "  eig, eiv = laplacian_eig(adj, device=device)\n",
        "  nodes = eiv[:, eig[eig > 0].argmin()].flatten()\n",
        "\n",
        "  adj_list = []\n",
        "  p = np.array_split(np.argsort(nodes), k)\n",
        "  for i, part in enumerate(p):\n",
        "    if remove_edges:\n",
        "      A = adj.copy()\n",
        "      # All nodes not present in the current partition\n",
        "      not_p = np.hstack(np.delete(p, i, axis=0))\n",
        "      A[not_p,:] = 0\n",
        "      A[:,not_p] = 0\n",
        "      A.eliminate_zeros()\n",
        "    else:\n",
        "      A = sp.dok_matrix(adj.shape, dtype=np.float32).tocsr()\n",
        "      A[part,:] = adj[part,:]\n",
        "      A[:,part] = adj[:,part]\n",
        "      A.eliminate_zeros()\n",
        "    adj_list.append(A)\n",
        "  return adj_list, p\n",
        "\n",
        "def random_edge_splitting(adj, k):\n",
        "  adj = sp.triu(adj)\n",
        "  if not sp.isspmatrix_coo(adj):\n",
        "    adj = adj.tocoo()\n",
        "  edges_idx = np.arange(adj.sum())\n",
        "  np.random.shuffle(edges_idx)\n",
        "  p = np.array_split(edges_idx, k)\n",
        "  adj_list = []\n",
        "  for part in p:\n",
        "    part = part.astype(np.int32)\n",
        "    rows = np.hstack((adj.row[part], adj.col[part]))\n",
        "    cols = np.hstack((adj.col[part], adj.row[part]))\n",
        "    A = sp.csr_matrix((np.ones(len(part) * 2), (rows, cols)), adj.shape, dtype=np.float32)\n",
        "    adj_list.append(A)\n",
        "  return adj_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run experiments"
      ],
      "metadata": {
        "id": "hZ957kRs0IFk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LYmmGLQrGaY"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import json\n",
        "dev = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "DATASET = 'pubmed'\n",
        "if DATASET == 'facebook':\n",
        "  with open('facebook.json') as f:\n",
        "    data = json.load(f)\n",
        "  X = torch.Tensor(data['feat'])\n",
        "  edges = np.asarray(data['edges'])\n",
        "  A = sp.csr_matrix((np.ones(len(edges)), (edges[:,0], edges[:,1])), (X.shape[0], X.shape[0]), dtype=np.float32)\n",
        "else:\n",
        "  if DATASET == 'cora':\n",
        "    g = dgl.data.CoraGraphDataset()[0]\n",
        "  elif DATASET == 'citeseer':\n",
        "    g = dgl.data.CiteseerGraphDataset()[0]\n",
        "  elif DATASET == 'pubmed':\n",
        "    g = dgl.data.PubmedGraphDataset()[0]\n",
        "  # adjacency matrix\n",
        "  A = g.adj_external(scipy_fmt='csr').astype(np.float32)\n",
        "  # feature vectors\n",
        "  X = g.ndata['feat'].to(dev)\n",
        "\n",
        "m1 = GCAE(X.shape[1], 128, 64)\n",
        "m2 = GNCAE(X.shape[1], 128, 64, s=1.8)\n",
        "m3 = GATAE(X.shape[1], 128, 64, 4, 1)\n",
        "for model in [m1]:\n",
        "  model = model.to(dev)\n",
        "  metrics = train_contrastive_kfold(model, A, X, dgl_model=True, part_type='clust_conj', patience_max=50, dev=dev)\n",
        "  with open(f'{DATASET}_{model._get_name()}_metrics_contrastive.pkl', 'wb') as f:\n",
        "    pickle.dump(metrics, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Print saved results in tabular format"
      ],
      "metadata": {
        "id": "f5bd5c1F0puf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hsMNOtMCtS7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "DATASET = 'cora'\n",
        "for f in os.listdir(DATASET):\n",
        "  with open(os.path.join(DATASET, f), 'rb') as f:\n",
        "    mtr = pickle.load(f)\n",
        "  auc = np.array([x['auc'] for x in mtr])\n",
        "  ap = np.array([x['ap'] for x in mtr])\n",
        "  mrr = np.array([x['mrr'] for x in mtr])\n",
        "  mr = np.array([x['mr'] for x in mtr])\n",
        "  h1 = np.array([x['hits1'] for x in mtr])\n",
        "  h3 = np.array([x['hits3'] for x in mtr])\n",
        "  h10 = np.array([x['hits10'] for x in mtr])\n",
        "  print(f.name)\n",
        "  print(f'{auc.mean():.3f} {auc.std():.2f}\\t{ap.mean():.3f} {ap.std():.2f}\\t{mrr.mean():.3f} {mrr.std():.2f}\\t{mr.mean():.3f} {mr.std():.2f}\\t{h1.mean():.3f} {h1.std():.2f}\\t{h3.mean():.3f} {h3.std():.2f}\\t{h10.mean():.3f} {h10.std():.2f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}